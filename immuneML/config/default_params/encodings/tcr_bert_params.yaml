model: "tcr-bert" # huggingface model hub identifier
layers: [-1] # Transformer layer to use. Layers should be given as negative integers, where -1 indicates the last
    # representation, -2 second to last, etc.
method: "mean" # which method to use to pool the embeddings, see TCR-BERT paper for more details
batch_size: 4096 # number of sequences to load at once
device: 'cpu'